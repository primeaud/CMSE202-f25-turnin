{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"text-align: right;\"> &#9989; Dylan Primeau</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMSE 202 Final (Section 002 - Spring 2025) (80 points total)\n",
    "\n",
    "The goal of this final is to give you the opportunity to test out some of the skills that you've developed having now finished CMSE 202. In particular, you'll be committing and pushing repository changes to a GitHub repository, working with data to build a network graph, performing regression analysis, and classifying data using a machine learning classifier. You should find that you have all of the skills necessary to complete this exam having completed the second half of CMSE 202!\n",
    "\n",
    "You are encouraged to look through the entire exam before you get started so that you can appropriately budget your time and understand the broad goals of the exam. Once you've read through it, you'll probably want to make sure you do Part 1 first to ensure that your GitHub repository is working correctly. Let your instructor know right away if you run into issues!\n",
    "\n",
    "**Important note about using online resources**: This exam is \"open internet\". That means that you can look up documentation, google how to accomplish certain Python tasks, etc. Being able to effectively use the internet for computational modeling and data science is a very important skill, so we want to make sure you have the opportunity to exercise that skill. You can also use _your version_ of past CMSE 202 assignments and the CMSE 202 course materials as a resource! **However: The use of any person-to-person communication software or is absolutely not acceptable.** If you are seen accessing your email, using a collaborative cloud storage or document software (e.g. Slack, Google Documents) you will be at risk for receiving a zero on the exam. Regardless of what resources you use whether they are ai or stackexchange, CITE THEM PLEASE. You will get marked down if you provide a response that looks copy and pasted without a citation. If you are unsure of how to do a citation or if your citation is good enough, ask your professor or instructional staff for help.\n",
    "\n",
    "**Keep your eyes on your screen!** Unfortunately, there isn't enough space in the room for everyone to sit at their own table so please do your best to keep your eyes on your own screen. This exam is designed to give *you* the opportunity to show the instructor what you can do and you should hold yourself accountable for maintaining a high level of academic integrity. If any of the instructors observe suspicious behavior, you will, again, risk receiving a zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Part 0: Academic integrity statement\n",
    "\n",
    "Read the following statement and edit the markdown text to put your name in the statement. This is your commitment to doing your own authentic work on this exam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I, Dylan Primeau, affirm that this exam represents my own authetic work, without the use of any unpermitted aids or resources or person-to-person communication. I understand that this exam an an opportunity to showcase my own progress in developing and improving my computational skills and have done my best to demonstrate those skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Part 1: Add to your Git repository to track your progress on your exam (7 points)\n",
    "\n",
    "Before you get to far along in the exam, you're going to add it to the `cmse202-s25-turnin` repository you created in class so that you can track your progress on the exam and preserve the final version that you turn in. In order to do this you need to\n",
    "\n",
    "**&#9989; Do the following**:\n",
    "\n",
    "1. Navigate to your `cmse202-s25-turnin` repository and create a new directory called `final`. You may have already done this in the midterm. If you have, then good job move on to the next part!\n",
    "2. Move this notebook into that **new directory** in your repository, then **add it and commit it to your repository**.\n",
    "1. Finally, to test that everything is working, \"git push\" the file so that it ends up in your GitHub repository.\n",
    "\n",
    "**Important**: Double check you've added your Professor and your TA as collaborators to your \"turnin\" respository (you should have done this previously).\n",
    "\n",
    "**Also important**: Make sure that the version of this notebook that you are working on is the same one that you just added to your repository! If you are working on a different copy of the noteobok, **none of your changes will be tracked**!\n",
    "\n",
    "If everything went as intended, the file should now show up on your GitHub account in the \"`cmse202-s25-turnin`\" repository inside the `final` directory that you just created. Periodically, **you'll be asked to commit your changes to the repository and push them to the remote GitHub location**. Of course, you can always commit your changes more often than that, if you wish.  It can be good to get into a habit of committing your changes any time you make a significant modification, or when you stop working on the project for a bit.\n",
    "\n",
    "&#9989; **Do this**: Before you move on, put the command that your instructor should run to clone your repository in the markdown cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` bash\n",
    "# Put the command for cloning your repository here!\n",
    "git clone https://github.com/primeaud/CMSE202-f25-turnin.git\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Part 2: Graph Theory and Networks (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 2.1 (9 points)**: We want you to make a very basic model of the personal network you have built this term. Using Networkx, visualize a network that has at least five (including yourself) CMSE202 classmates you have met and worked with this term. There should be an edge connecting you to all of the other classmates. Add at least one edge between two classmates that know each other (besides yourself). If you do not know names, use your group project member names to make this graph. The Day 10 ICA and PCA should help with doing this. I reccomend using dictionaries to make your network. See if you can display the names of this network on your diagram as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Put your code for Question 2.1 here\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "graph = nx.Graph()\n",
    "group_members = [\"Dylan\", \"Declan\", \"Josh\", \"Noah\", \"Jack\", \"Owen\"]\n",
    "graph.add_nodes_from(group_members)\n",
    "connections = [(\"Dylan\", member) for member in group_members if member != \"Dylan\"]\n",
    "connections.append((\"Noah\", \"Jack\"))\n",
    "graph.add_edges_from(connections)\n",
    "nx.draw(graph, with_labels=True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository!**\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository using the commit message \"**Committing Part 2**\", and push the changes to GitHub.\n",
    "\n",
    "\n",
    "If committing and/or pushing isn't working for you, write down the complete commands in this cell that would have committed your changes (with the commit message) and pushed them to GitHub.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Part 3: Regression Analysis on Data (27 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal for this part is to do a multivariate linear regression, also known as multiple linear regression, to understand the relation between renting a bike and other factors. You can download the dataset from this URL:\n",
    "\n",
    "`https://raw.githubusercontent.com/gambre11/CMSE202/refs/heads/main/day.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 3.1 (3 points)**: To get started, **download the `day.csv` file and place it in the same directory as your notebook**, then **read in the `day.csv` dataset and name it `bike_data`** and finally **display the first few rows of the data**. You can use `Pandas` for this task or any other Python tool you prefer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my terminal using CLI:\n",
    "\n",
    "curl -O https://raw.githubusercontent.com/gambre11/CMSE202/main/day.csv\n",
    "mv day.csv ~/Downloads/CMSE202-f25-turnin/cmse202-s25-turnin/exams/final/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.344167</td>\n",
       "      <td>0.363625</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.160446</td>\n",
       "      <td>331</td>\n",
       "      <td>654</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.363478</td>\n",
       "      <td>0.353739</td>\n",
       "      <td>0.696087</td>\n",
       "      <td>0.248539</td>\n",
       "      <td>131</td>\n",
       "      <td>670</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196364</td>\n",
       "      <td>0.189405</td>\n",
       "      <td>0.437273</td>\n",
       "      <td>0.248309</td>\n",
       "      <td>120</td>\n",
       "      <td>1229</td>\n",
       "      <td>1349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-04</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.212122</td>\n",
       "      <td>0.590435</td>\n",
       "      <td>0.160296</td>\n",
       "      <td>108</td>\n",
       "      <td>1454</td>\n",
       "      <td>1562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-05</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.226957</td>\n",
       "      <td>0.229270</td>\n",
       "      <td>0.436957</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>82</td>\n",
       "      <td>1518</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instant      dteday  season  yr  mnth  holiday  weekday  workingday  \\\n",
       "0        1  2011-01-01       1   0     1        0        6           0   \n",
       "1        2  2011-01-02       1   0     1        0        0           0   \n",
       "2        3  2011-01-03       1   0     1        0        1           1   \n",
       "3        4  2011-01-04       1   0     1        0        2           1   \n",
       "4        5  2011-01-05       1   0     1        0        3           1   \n",
       "\n",
       "   weathersit      temp     atemp       hum  windspeed  casual  registered  \\\n",
       "0           2  0.344167  0.363625  0.805833   0.160446     331         654   \n",
       "1           2  0.363478  0.353739  0.696087   0.248539     131         670   \n",
       "2           1  0.196364  0.189405  0.437273   0.248309     120        1229   \n",
       "3           1  0.200000  0.212122  0.590435   0.160296     108        1454   \n",
       "4           1  0.226957  0.229270  0.436957   0.186900      82        1518   \n",
       "\n",
       "    cnt  \n",
       "0   985  \n",
       "1   801  \n",
       "2  1349  \n",
       "3  1562  \n",
       "4  1600  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Put your code here\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "bike_data = pd.read_csv(\"day.csv\")\n",
    "bike_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information for this dataset can be found here: https://www.kaggle.com/datasets/lakshmi25npathi/bike-sharing-dataset/data\n",
    "\n",
    "You will be trying to predict `cnt` (which is the number of bike rentals) using linear regression with the features.\n",
    "\n",
    "\n",
    "&#9989; **Question 3.2 (2 points)**: Remove the columns `dteday`, `instant`,`casual`, and `registered` from the data you loaded in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.344167</td>\n",
       "      <td>0.363625</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.160446</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.363478</td>\n",
       "      <td>0.353739</td>\n",
       "      <td>0.696087</td>\n",
       "      <td>0.248539</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196364</td>\n",
       "      <td>0.189405</td>\n",
       "      <td>0.437273</td>\n",
       "      <td>0.248309</td>\n",
       "      <td>1349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.212122</td>\n",
       "      <td>0.590435</td>\n",
       "      <td>0.160296</td>\n",
       "      <td>1562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.226957</td>\n",
       "      <td>0.229270</td>\n",
       "      <td>0.436957</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   season  yr  mnth  holiday  weekday  workingday  weathersit      temp  \\\n",
       "0       1   0     1        0        6           0           2  0.344167   \n",
       "1       1   0     1        0        0           0           2  0.363478   \n",
       "2       1   0     1        0        1           1           1  0.196364   \n",
       "3       1   0     1        0        2           1           1  0.200000   \n",
       "4       1   0     1        0        3           1           1  0.226957   \n",
       "\n",
       "      atemp       hum  windspeed   cnt  \n",
       "0  0.363625  0.805833   0.160446   985  \n",
       "1  0.353739  0.696087   0.248539   801  \n",
       "2  0.189405  0.437273   0.248309  1349  \n",
       "3  0.212122  0.590435   0.160296  1562  \n",
       "4  0.229270  0.436957   0.186900  1600  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Put your code here\n",
    "\n",
    "bike_data = bike_data.drop(columns=[\"dteday\", \"instant\", \"casual\", \"registered\"])\n",
    "bike_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; Run this following code cell. This is taking some columns and separating the data. You will likely need to change the variable name to match the variable name of your data set though so pay attention to everywhere it mentions `bike_new`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "bike_data['season'] = bike_data['season'].astype('category')\n",
    "bike_data['weathersit'] = bike_data['weathersit'].astype('category')\n",
    "bike_data['mnth'] = bike_data['mnth'].astype('category')\n",
    "bike_data['weekday'] = bike_data['weekday'].astype('category')\n",
    "\n",
    "bike_data = pd.get_dummies(bike_data, drop_first=True)\n",
    "bike_data.replace({False: 0, True: 1}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 3.3 (3 points)**: **Construct two data frames** using the loaded and cleaned data: one named `labels` and the other named `features`. The `labels` data frame should consist solely of the `cnt` column, while the `features` data frame should contain all the other columns. **Display the first few lines of these data frames.** Note the pandas `.pop` method may be helpful here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0     985\n",
       " 1     801\n",
       " 2    1349\n",
       " 3    1562\n",
       " 4    1600\n",
       " Name: cnt, dtype: int64,\n",
       "    yr  holiday  workingday      temp     atemp       hum  windspeed  season_2  \\\n",
       " 0   0        0           0  0.344167  0.363625  0.805833   0.160446         0   \n",
       " 1   0        0           0  0.363478  0.353739  0.696087   0.248539         0   \n",
       " 2   0        0           1  0.196364  0.189405  0.437273   0.248309         0   \n",
       " 3   0        0           1  0.200000  0.212122  0.590435   0.160296         0   \n",
       " 4   0        0           1  0.226957  0.229270  0.436957   0.186900         0   \n",
       " \n",
       "    season_3  season_4  ...  mnth_11  mnth_12  weekday_1  weekday_2  weekday_3  \\\n",
       " 0         0         0  ...        0        0          0          0          0   \n",
       " 1         0         0  ...        0        0          0          0          0   \n",
       " 2         0         0  ...        0        0          1          0          0   \n",
       " 3         0         0  ...        0        0          0          1          0   \n",
       " 4         0         0  ...        0        0          0          0          1   \n",
       " \n",
       "    weekday_4  weekday_5  weekday_6  weathersit_2  weathersit_3  \n",
       " 0          0          0          1             1             0  \n",
       " 1          0          0          0             1             0  \n",
       " 2          0          0          0             0             0  \n",
       " 3          0          0          0             0             0  \n",
       " 4          0          0          0             0             0  \n",
       " \n",
       " [5 rows x 29 columns])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Put your code here\n",
    "labels = bike_data.pop('cnt')\n",
    "features = bike_data\n",
    "labels.head(), features.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will fit the data using the ordinary least squares model `OLS` in `statsmodel`. \n",
    "\n",
    "&#9989; **Question 3.4 (2 point)**: Before proceeding, **add a column of constants** (set to 1.0) to the `features` data frame. You learned about a `statsmodels` function in class that can accomplish this task. Label the modified data frame as `features_const`. **Display** `features_const` to verify that the new column is indeed added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>const</th>\n",
       "      <th>yr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>season_2</th>\n",
       "      <th>season_3</th>\n",
       "      <th>...</th>\n",
       "      <th>mnth_11</th>\n",
       "      <th>mnth_12</th>\n",
       "      <th>weekday_1</th>\n",
       "      <th>weekday_2</th>\n",
       "      <th>weekday_3</th>\n",
       "      <th>weekday_4</th>\n",
       "      <th>weekday_5</th>\n",
       "      <th>weekday_6</th>\n",
       "      <th>weathersit_2</th>\n",
       "      <th>weathersit_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.344167</td>\n",
       "      <td>0.363625</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.160446</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.363478</td>\n",
       "      <td>0.353739</td>\n",
       "      <td>0.696087</td>\n",
       "      <td>0.248539</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196364</td>\n",
       "      <td>0.189405</td>\n",
       "      <td>0.437273</td>\n",
       "      <td>0.248309</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.212122</td>\n",
       "      <td>0.590435</td>\n",
       "      <td>0.160296</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.226957</td>\n",
       "      <td>0.229270</td>\n",
       "      <td>0.436957</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   const  yr  holiday  workingday      temp     atemp       hum  windspeed  \\\n",
       "0    1.0   0        0           0  0.344167  0.363625  0.805833   0.160446   \n",
       "1    1.0   0        0           0  0.363478  0.353739  0.696087   0.248539   \n",
       "2    1.0   0        0           1  0.196364  0.189405  0.437273   0.248309   \n",
       "3    1.0   0        0           1  0.200000  0.212122  0.590435   0.160296   \n",
       "4    1.0   0        0           1  0.226957  0.229270  0.436957   0.186900   \n",
       "\n",
       "   season_2  season_3  ...  mnth_11  mnth_12  weekday_1  weekday_2  weekday_3  \\\n",
       "0         0         0  ...        0        0          0          0          0   \n",
       "1         0         0  ...        0        0          0          0          0   \n",
       "2         0         0  ...        0        0          1          0          0   \n",
       "3         0         0  ...        0        0          0          1          0   \n",
       "4         0         0  ...        0        0          0          0          1   \n",
       "\n",
       "   weekday_4  weekday_5  weekday_6  weathersit_2  weathersit_3  \n",
       "0          0          0          1             1             0  \n",
       "1          0          0          0             1             0  \n",
       "2          0          0          0             0             0  \n",
       "3          0          0          0             0             0  \n",
       "4          0          0          0             0             0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Put your code here\n",
    "import statsmodels.api as sm\n",
    "features_const = sm.add_constant(features)\n",
    "features_const.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will perform the actual fit.\n",
    "\n",
    "&#9989; **Question 3.5 (4 points)**: Using `statsmodels` `OLS`, **perform a fit** using `labels` (containing `cnt`) as the dependent variable and `features_const` as the independent variable. **Print the fit** using `summary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>cnt</td>       <th>  R-squared:         </th> <td>   0.848</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.842</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   140.3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 01 May 2025</td> <th>  Prob (F-statistic):</th> <td>8.67e-266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>08:30:22</td>     <th>  Log-Likelihood:    </th> <td> -5880.2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   731</td>      <th>  AIC:               </th> <td>1.182e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   702</td>      <th>  BIC:               </th> <td>1.195e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    28</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>        <td> 1485.8439</td> <td>  239.746</td> <td>    6.198</td> <td> 0.000</td> <td> 1015.138</td> <td> 1956.550</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>yr</th>           <td> 2019.7354</td> <td>   58.220</td> <td>   34.691</td> <td> 0.000</td> <td> 1905.429</td> <td> 2134.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>holiday</th>      <td> -258.8454</td> <td>  160.940</td> <td>   -1.608</td> <td> 0.108</td> <td> -574.827</td> <td>   57.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>workingday</th>   <td>  330.8518</td> <td>   66.825</td> <td>    4.951</td> <td> 0.000</td> <td>  199.651</td> <td>  462.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>temp</th>         <td> 2855.0107</td> <td> 1398.156</td> <td>    2.042</td> <td> 0.042</td> <td>  109.942</td> <td> 5600.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>atemp</th>        <td> 1786.1574</td> <td> 1462.120</td> <td>    1.222</td> <td> 0.222</td> <td>-1084.494</td> <td> 4656.808</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hum</th>          <td>-1535.4684</td> <td>  292.448</td> <td>   -5.250</td> <td> 0.000</td> <td>-2109.646</td> <td> -961.290</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>windspeed</th>    <td>-2823.2967</td> <td>  414.552</td> <td>   -6.810</td> <td> 0.000</td> <td>-3637.207</td> <td>-2009.386</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>season_2</th>     <td>  884.7108</td> <td>  179.492</td> <td>    4.929</td> <td> 0.000</td> <td>  532.305</td> <td> 1237.117</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>season_3</th>     <td>  832.7022</td> <td>  213.129</td> <td>    3.907</td> <td> 0.000</td> <td>  414.255</td> <td> 1251.150</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>season_4</th>     <td> 1575.3506</td> <td>  181.001</td> <td>    8.704</td> <td> 0.000</td> <td> 1219.983</td> <td> 1930.718</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth_2</th>       <td>  131.0251</td> <td>  143.776</td> <td>    0.911</td> <td> 0.362</td> <td> -151.258</td> <td>  413.308</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth_3</th>       <td>  542.8277</td> <td>  165.433</td> <td>    3.281</td> <td> 0.001</td> <td>  218.025</td> <td>  867.630</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth_4</th>       <td>  451.1656</td> <td>  247.567</td> <td>    1.822</td> <td> 0.069</td> <td>  -34.895</td> <td>  937.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth_5</th>       <td>  735.5055</td> <td>  267.628</td> <td>    2.748</td> <td> 0.006</td> <td>  210.059</td> <td> 1260.952</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth_6</th>       <td>  515.4048</td> <td>  282.411</td> <td>    1.825</td> <td> 0.068</td> <td>  -39.066</td> <td> 1069.876</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth_7</th>       <td>   30.7966</td> <td>  313.821</td> <td>    0.098</td> <td> 0.922</td> <td> -585.343</td> <td>  646.937</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth_8</th>       <td>  444.9490</td> <td>  303.165</td> <td>    1.468</td> <td> 0.143</td> <td> -150.270</td> <td> 1040.168</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth_9</th>       <td> 1004.1728</td> <td>  265.123</td> <td>    3.788</td> <td> 0.000</td> <td>  483.644</td> <td> 1524.701</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth_10</th>      <td>  519.6743</td> <td>  241.554</td> <td>    2.151</td> <td> 0.032</td> <td>   45.420</td> <td>  993.928</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth_11</th>      <td> -116.6933</td> <td>  230.776</td> <td>   -0.506</td> <td> 0.613</td> <td> -569.788</td> <td>  336.401</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth_12</th>      <td>  -89.5915</td> <td>  182.214</td> <td>   -0.492</td> <td> 0.623</td> <td> -447.342</td> <td>  268.159</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weekday_1</th>    <td> -118.7980</td> <td>   71.469</td> <td>   -1.662</td> <td> 0.097</td> <td> -259.116</td> <td>   21.520</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weekday_2</th>    <td>  -21.3242</td> <td>   77.324</td> <td>   -0.276</td> <td> 0.783</td> <td> -173.138</td> <td>  130.490</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weekday_3</th>    <td>   50.5056</td> <td>   77.670</td> <td>    0.650</td> <td> 0.516</td> <td> -101.988</td> <td>  202.999</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weekday_4</th>    <td>   55.4911</td> <td>   77.017</td> <td>    0.721</td> <td> 0.471</td> <td>  -95.721</td> <td>  206.703</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weekday_5</th>    <td>  106.1319</td> <td>   77.308</td> <td>    1.373</td> <td> 0.170</td> <td>  -45.652</td> <td>  257.915</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weekday_6</th>    <td>  440.4588</td> <td>  106.562</td> <td>    4.133</td> <td> 0.000</td> <td>  231.240</td> <td>  649.678</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weathersit_2</th> <td> -462.5381</td> <td>   77.087</td> <td>   -6.000</td> <td> 0.000</td> <td> -613.887</td> <td> -311.190</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weathersit_3</th> <td>-1965.0865</td> <td>  197.052</td> <td>   -9.972</td> <td> 0.000</td> <td>-2351.968</td> <td>-1578.205</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>125.281</td> <th>  Durbin-Watson:     </th> <td>   1.198</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 347.309</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.860</td>  <th>  Prob(JB):          </th> <td>3.83e-76</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.906</td>  <th>  Cond. No.          </th> <td>3.85e+15</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 1.6e-28. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &       cnt        & \\textbf{  R-squared:         } &     0.848   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.842   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     140.3   \\\\\n",
       "\\textbf{Date:}             & Thu, 01 May 2025 & \\textbf{  Prob (F-statistic):} & 8.67e-266   \\\\\n",
       "\\textbf{Time:}             &     08:30:22     & \\textbf{  Log-Likelihood:    } &   -5880.2   \\\\\n",
       "\\textbf{No. Observations:} &         731      & \\textbf{  AIC:               } & 1.182e+04   \\\\\n",
       "\\textbf{Df Residuals:}     &         702      & \\textbf{  BIC:               } & 1.195e+04   \\\\\n",
       "\\textbf{Df Model:}         &          28      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                       & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}         &    1485.8439  &      239.746     &     6.198  &         0.000        &     1015.138    &     1956.550     \\\\\n",
       "\\textbf{yr}            &    2019.7354  &       58.220     &    34.691  &         0.000        &     1905.429    &     2134.042     \\\\\n",
       "\\textbf{holiday}       &    -258.8454  &      160.940     &    -1.608  &         0.108        &     -574.827    &       57.136     \\\\\n",
       "\\textbf{workingday}    &     330.8518  &       66.825     &     4.951  &         0.000        &      199.651    &      462.053     \\\\\n",
       "\\textbf{temp}          &    2855.0107  &     1398.156     &     2.042  &         0.042        &      109.942    &     5600.079     \\\\\n",
       "\\textbf{atemp}         &    1786.1574  &     1462.120     &     1.222  &         0.222        &    -1084.494    &     4656.808     \\\\\n",
       "\\textbf{hum}           &   -1535.4684  &      292.448     &    -5.250  &         0.000        &    -2109.646    &     -961.290     \\\\\n",
       "\\textbf{windspeed}     &   -2823.2967  &      414.552     &    -6.810  &         0.000        &    -3637.207    &    -2009.386     \\\\\n",
       "\\textbf{season\\_2}     &     884.7108  &      179.492     &     4.929  &         0.000        &      532.305    &     1237.117     \\\\\n",
       "\\textbf{season\\_3}     &     832.7022  &      213.129     &     3.907  &         0.000        &      414.255    &     1251.150     \\\\\n",
       "\\textbf{season\\_4}     &    1575.3506  &      181.001     &     8.704  &         0.000        &     1219.983    &     1930.718     \\\\\n",
       "\\textbf{mnth\\_2}       &     131.0251  &      143.776     &     0.911  &         0.362        &     -151.258    &      413.308     \\\\\n",
       "\\textbf{mnth\\_3}       &     542.8277  &      165.433     &     3.281  &         0.001        &      218.025    &      867.630     \\\\\n",
       "\\textbf{mnth\\_4}       &     451.1656  &      247.567     &     1.822  &         0.069        &      -34.895    &      937.226     \\\\\n",
       "\\textbf{mnth\\_5}       &     735.5055  &      267.628     &     2.748  &         0.006        &      210.059    &     1260.952     \\\\\n",
       "\\textbf{mnth\\_6}       &     515.4048  &      282.411     &     1.825  &         0.068        &      -39.066    &     1069.876     \\\\\n",
       "\\textbf{mnth\\_7}       &      30.7966  &      313.821     &     0.098  &         0.922        &     -585.343    &      646.937     \\\\\n",
       "\\textbf{mnth\\_8}       &     444.9490  &      303.165     &     1.468  &         0.143        &     -150.270    &     1040.168     \\\\\n",
       "\\textbf{mnth\\_9}       &    1004.1728  &      265.123     &     3.788  &         0.000        &      483.644    &     1524.701     \\\\\n",
       "\\textbf{mnth\\_10}      &     519.6743  &      241.554     &     2.151  &         0.032        &       45.420    &      993.928     \\\\\n",
       "\\textbf{mnth\\_11}      &    -116.6933  &      230.776     &    -0.506  &         0.613        &     -569.788    &      336.401     \\\\\n",
       "\\textbf{mnth\\_12}      &     -89.5915  &      182.214     &    -0.492  &         0.623        &     -447.342    &      268.159     \\\\\n",
       "\\textbf{weekday\\_1}    &    -118.7980  &       71.469     &    -1.662  &         0.097        &     -259.116    &       21.520     \\\\\n",
       "\\textbf{weekday\\_2}    &     -21.3242  &       77.324     &    -0.276  &         0.783        &     -173.138    &      130.490     \\\\\n",
       "\\textbf{weekday\\_3}    &      50.5056  &       77.670     &     0.650  &         0.516        &     -101.988    &      202.999     \\\\\n",
       "\\textbf{weekday\\_4}    &      55.4911  &       77.017     &     0.721  &         0.471        &      -95.721    &      206.703     \\\\\n",
       "\\textbf{weekday\\_5}    &     106.1319  &       77.308     &     1.373  &         0.170        &      -45.652    &      257.915     \\\\\n",
       "\\textbf{weekday\\_6}    &     440.4588  &      106.562     &     4.133  &         0.000        &      231.240    &      649.678     \\\\\n",
       "\\textbf{weathersit\\_2} &    -462.5381  &       77.087     &    -6.000  &         0.000        &     -613.887    &     -311.190     \\\\\n",
       "\\textbf{weathersit\\_3} &   -1965.0865  &      197.052     &    -9.972  &         0.000        &    -2351.968    &    -1578.205     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 125.281 & \\textbf{  Durbin-Watson:     } &    1.198  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } &  347.309  \\\\\n",
       "\\textbf{Skew:}          &  -0.860 & \\textbf{  Prob(JB):          } & 3.83e-76  \\\\\n",
       "\\textbf{Kurtosis:}      &   5.906 & \\textbf{  Cond. No.          } & 3.85e+15  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The smallest eigenvalue is 1.6e-28. This might indicate that there are \\newline\n",
       " strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    cnt   R-squared:                       0.848\n",
       "Model:                            OLS   Adj. R-squared:                  0.842\n",
       "Method:                 Least Squares   F-statistic:                     140.3\n",
       "Date:                Thu, 01 May 2025   Prob (F-statistic):          8.67e-266\n",
       "Time:                        08:30:22   Log-Likelihood:                -5880.2\n",
       "No. Observations:                 731   AIC:                         1.182e+04\n",
       "Df Residuals:                     702   BIC:                         1.195e+04\n",
       "Df Model:                          28                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "================================================================================\n",
       "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------\n",
       "const         1485.8439    239.746      6.198      0.000    1015.138    1956.550\n",
       "yr            2019.7354     58.220     34.691      0.000    1905.429    2134.042\n",
       "holiday       -258.8454    160.940     -1.608      0.108    -574.827      57.136\n",
       "workingday     330.8518     66.825      4.951      0.000     199.651     462.053\n",
       "temp          2855.0107   1398.156      2.042      0.042     109.942    5600.079\n",
       "atemp         1786.1574   1462.120      1.222      0.222   -1084.494    4656.808\n",
       "hum          -1535.4684    292.448     -5.250      0.000   -2109.646    -961.290\n",
       "windspeed    -2823.2967    414.552     -6.810      0.000   -3637.207   -2009.386\n",
       "season_2       884.7108    179.492      4.929      0.000     532.305    1237.117\n",
       "season_3       832.7022    213.129      3.907      0.000     414.255    1251.150\n",
       "season_4      1575.3506    181.001      8.704      0.000    1219.983    1930.718\n",
       "mnth_2         131.0251    143.776      0.911      0.362    -151.258     413.308\n",
       "mnth_3         542.8277    165.433      3.281      0.001     218.025     867.630\n",
       "mnth_4         451.1656    247.567      1.822      0.069     -34.895     937.226\n",
       "mnth_5         735.5055    267.628      2.748      0.006     210.059    1260.952\n",
       "mnth_6         515.4048    282.411      1.825      0.068     -39.066    1069.876\n",
       "mnth_7          30.7966    313.821      0.098      0.922    -585.343     646.937\n",
       "mnth_8         444.9490    303.165      1.468      0.143    -150.270    1040.168\n",
       "mnth_9        1004.1728    265.123      3.788      0.000     483.644    1524.701\n",
       "mnth_10        519.6743    241.554      2.151      0.032      45.420     993.928\n",
       "mnth_11       -116.6933    230.776     -0.506      0.613    -569.788     336.401\n",
       "mnth_12        -89.5915    182.214     -0.492      0.623    -447.342     268.159\n",
       "weekday_1     -118.7980     71.469     -1.662      0.097    -259.116      21.520\n",
       "weekday_2      -21.3242     77.324     -0.276      0.783    -173.138     130.490\n",
       "weekday_3       50.5056     77.670      0.650      0.516    -101.988     202.999\n",
       "weekday_4       55.4911     77.017      0.721      0.471     -95.721     206.703\n",
       "weekday_5      106.1319     77.308      1.373      0.170     -45.652     257.915\n",
       "weekday_6      440.4588    106.562      4.133      0.000     231.240     649.678\n",
       "weathersit_2  -462.5381     77.087     -6.000      0.000    -613.887    -311.190\n",
       "weathersit_3 -1965.0865    197.052     -9.972      0.000   -2351.968   -1578.205\n",
       "==============================================================================\n",
       "Omnibus:                      125.281   Durbin-Watson:                   1.198\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              347.309\n",
       "Skew:                          -0.860   Prob(JB):                     3.83e-76\n",
       "Kurtosis:                       5.906   Cond. No.                     3.85e+15\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 1.6e-28. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Put your code here\n",
    "model = sm.OLS(labels, features_const)\n",
    "results = model.fit()\n",
    "results.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 3.5 (2 points)**: Among all the features, is there one or more that are significantly less important than others? justify your answer with a sentence or two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> mnth_7(p=0.922), mnth_11(p=0.613), and weekday_2(p=0.783) have p vals higher than 0.05, menaning their coefficients are not statistically different from zero and may not be useful predictors in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 3.6 (3 points)**: **Remove the \"least important\" features**, then **fit the model again** and **print the fit** using `summary()` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>cnt</td>       <th>  R-squared:         </th> <td>   0.848</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.843</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   157.6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 01 May 2025</td> <th>  Prob (F-statistic):</th> <td>5.72e-269</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>08:39:33</td>     <th>  Log-Likelihood:    </th> <td> -5880.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   731</td>      <th>  AIC:               </th> <td>1.181e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   705</td>      <th>  BIC:               </th> <td>1.193e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    25</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>        <td> 1467.0996</td> <td>  237.017</td> <td>    6.190</td> <td> 0.000</td> <td> 1001.755</td> <td> 1932.444</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>yr</th>           <td> 2017.5574</td> <td>   57.805</td> <td>   34.903</td> <td> 0.000</td> <td> 1904.067</td> <td> 2131.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>holiday</th>      <td> -293.6274</td> <td>  206.548</td> <td>   -1.422</td> <td> 0.156</td> <td> -699.150</td> <td>  111.895</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>workingday</th>   <td>  307.8027</td> <td>  106.723</td> <td>    2.884</td> <td> 0.004</td> <td>   98.270</td> <td>  517.336</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>temp</th>         <td> 2994.1135</td> <td> 1349.265</td> <td>    2.219</td> <td> 0.027</td> <td>  345.055</td> <td> 5643.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>atemp</th>        <td> 1716.3066</td> <td> 1454.226</td> <td>    1.180</td> <td> 0.238</td> <td>-1138.825</td> <td> 4571.439</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hum</th>          <td>-1560.0279</td> <td>  287.087</td> <td>   -5.434</td> <td> 0.000</td> <td>-2123.676</td> <td> -996.380</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>windspeed</th>    <td>-2846.9942</td> <td>  411.603</td> <td>   -6.917</td> <td> 0.000</td> <td>-3655.108</td> <td>-2038.880</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>season_2</th>     <td>  887.3336</td> <td>  164.562</td> <td>    5.392</td> <td> 0.000</td> <td>  564.243</td> <td> 1210.424</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>season_3</th>     <td>  832.0276</td> <td>  169.183</td> <td>    4.918</td> <td> 0.000</td> <td>  499.864</td> <td> 1164.191</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>season_4</th>     <td> 1495.1711</td> <td>  114.763</td> <td>   13.028</td> <td> 0.000</td> <td> 1269.853</td> <td> 1720.489</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth_2</th>       <td>  150.2921</td> <td>  130.537</td> <td>    1.151</td> <td> 0.250</td> <td> -105.995</td> <td>  406.580</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth_3</th>       <td>  554.3180</td> <td>  133.061</td> <td>    4.166</td> <td> 0.000</td> <td>  293.075</td> <td>  815.561</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth_4</th>       <td>  455.8258</td> <td>  189.424</td> <td>    2.406</td> <td> 0.016</td> <td>   83.924</td> <td>  827.728</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth_5</th>       <td>  731.4736</td> <td>  192.963</td> <td>    3.791</td> <td> 0.000</td> <td>  352.622</td> <td> 1110.325</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth_6</th>       <td>  502.7560</td> <td>  164.346</td> <td>    3.059</td> <td> 0.002</td> <td>  180.089</td> <td>  825.423</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth_8</th>       <td>  432.9388</td> <td>  132.744</td> <td>    3.261</td> <td> 0.001</td> <td>  172.319</td> <td>  693.559</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth_9</th>       <td> 1022.8398</td> <td>  122.773</td> <td>    8.331</td> <td> 0.000</td> <td>  781.795</td> <td> 1263.884</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mnth_10</th>      <td>  606.9923</td> <td>  126.412</td> <td>    4.802</td> <td> 0.000</td> <td>  358.803</td> <td>  855.182</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weekday_1</th>    <td>  -95.6125</td> <td>  108.992</td> <td>   -0.877</td> <td> 0.381</td> <td> -309.601</td> <td>  118.376</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weekday_3</th>    <td>   70.7323</td> <td>  106.736</td> <td>    0.663</td> <td> 0.508</td> <td> -138.826</td> <td>  280.290</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weekday_4</th>    <td>   74.9387</td> <td>  106.757</td> <td>    0.702</td> <td> 0.483</td> <td> -134.662</td> <td>  284.539</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weekday_5</th>    <td>  126.0524</td> <td>  107.145</td> <td>    1.176</td> <td> 0.240</td> <td>  -84.310</td> <td>  336.415</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weekday_6</th>    <td>  438.3308</td> <td>  106.312</td> <td>    4.123</td> <td> 0.000</td> <td>  229.605</td> <td>  647.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weathersit_2</th> <td> -458.7666</td> <td>   76.567</td> <td>   -5.992</td> <td> 0.000</td> <td> -609.093</td> <td> -308.440</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>weathersit_3</th> <td>-1964.3498</td> <td>  196.429</td> <td>  -10.000</td> <td> 0.000</td> <td>-2350.005</td> <td>-1578.695</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>125.403</td> <th>  Durbin-Watson:     </th> <td>   1.194</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 345.658</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.862</td>  <th>  Prob(JB):          </th> <td>8.74e-76</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.894</td>  <th>  Cond. No.          </th> <td>    124.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &       cnt        & \\textbf{  R-squared:         } &     0.848   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.843   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     157.6   \\\\\n",
       "\\textbf{Date:}             & Thu, 01 May 2025 & \\textbf{  Prob (F-statistic):} & 5.72e-269   \\\\\n",
       "\\textbf{Time:}             &     08:39:33     & \\textbf{  Log-Likelihood:    } &   -5880.5   \\\\\n",
       "\\textbf{No. Observations:} &         731      & \\textbf{  AIC:               } & 1.181e+04   \\\\\n",
       "\\textbf{Df Residuals:}     &         705      & \\textbf{  BIC:               } & 1.193e+04   \\\\\n",
       "\\textbf{Df Model:}         &          25      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                       & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}         &    1467.0996  &      237.017     &     6.190  &         0.000        &     1001.755    &     1932.444     \\\\\n",
       "\\textbf{yr}            &    2017.5574  &       57.805     &    34.903  &         0.000        &     1904.067    &     2131.047     \\\\\n",
       "\\textbf{holiday}       &    -293.6274  &      206.548     &    -1.422  &         0.156        &     -699.150    &      111.895     \\\\\n",
       "\\textbf{workingday}    &     307.8027  &      106.723     &     2.884  &         0.004        &       98.270    &      517.336     \\\\\n",
       "\\textbf{temp}          &    2994.1135  &     1349.265     &     2.219  &         0.027        &      345.055    &     5643.172     \\\\\n",
       "\\textbf{atemp}         &    1716.3066  &     1454.226     &     1.180  &         0.238        &    -1138.825    &     4571.439     \\\\\n",
       "\\textbf{hum}           &   -1560.0279  &      287.087     &    -5.434  &         0.000        &    -2123.676    &     -996.380     \\\\\n",
       "\\textbf{windspeed}     &   -2846.9942  &      411.603     &    -6.917  &         0.000        &    -3655.108    &    -2038.880     \\\\\n",
       "\\textbf{season\\_2}     &     887.3336  &      164.562     &     5.392  &         0.000        &      564.243    &     1210.424     \\\\\n",
       "\\textbf{season\\_3}     &     832.0276  &      169.183     &     4.918  &         0.000        &      499.864    &     1164.191     \\\\\n",
       "\\textbf{season\\_4}     &    1495.1711  &      114.763     &    13.028  &         0.000        &     1269.853    &     1720.489     \\\\\n",
       "\\textbf{mnth\\_2}       &     150.2921  &      130.537     &     1.151  &         0.250        &     -105.995    &      406.580     \\\\\n",
       "\\textbf{mnth\\_3}       &     554.3180  &      133.061     &     4.166  &         0.000        &      293.075    &      815.561     \\\\\n",
       "\\textbf{mnth\\_4}       &     455.8258  &      189.424     &     2.406  &         0.016        &       83.924    &      827.728     \\\\\n",
       "\\textbf{mnth\\_5}       &     731.4736  &      192.963     &     3.791  &         0.000        &      352.622    &     1110.325     \\\\\n",
       "\\textbf{mnth\\_6}       &     502.7560  &      164.346     &     3.059  &         0.002        &      180.089    &      825.423     \\\\\n",
       "\\textbf{mnth\\_8}       &     432.9388  &      132.744     &     3.261  &         0.001        &      172.319    &      693.559     \\\\\n",
       "\\textbf{mnth\\_9}       &    1022.8398  &      122.773     &     8.331  &         0.000        &      781.795    &     1263.884     \\\\\n",
       "\\textbf{mnth\\_10}      &     606.9923  &      126.412     &     4.802  &         0.000        &      358.803    &      855.182     \\\\\n",
       "\\textbf{weekday\\_1}    &     -95.6125  &      108.992     &    -0.877  &         0.381        &     -309.601    &      118.376     \\\\\n",
       "\\textbf{weekday\\_3}    &      70.7323  &      106.736     &     0.663  &         0.508        &     -138.826    &      280.290     \\\\\n",
       "\\textbf{weekday\\_4}    &      74.9387  &      106.757     &     0.702  &         0.483        &     -134.662    &      284.539     \\\\\n",
       "\\textbf{weekday\\_5}    &     126.0524  &      107.145     &     1.176  &         0.240        &      -84.310    &      336.415     \\\\\n",
       "\\textbf{weekday\\_6}    &     438.3308  &      106.312     &     4.123  &         0.000        &      229.605    &      647.057     \\\\\n",
       "\\textbf{weathersit\\_2} &    -458.7666  &       76.567     &    -5.992  &         0.000        &     -609.093    &     -308.440     \\\\\n",
       "\\textbf{weathersit\\_3} &   -1964.3498  &      196.429     &   -10.000  &         0.000        &    -2350.005    &    -1578.695     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 125.403 & \\textbf{  Durbin-Watson:     } &    1.194  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } &  345.658  \\\\\n",
       "\\textbf{Skew:}          &  -0.862 & \\textbf{  Prob(JB):          } & 8.74e-76  \\\\\n",
       "\\textbf{Kurtosis:}      &   5.894 & \\textbf{  Cond. No.          } &     124.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    cnt   R-squared:                       0.848\n",
       "Model:                            OLS   Adj. R-squared:                  0.843\n",
       "Method:                 Least Squares   F-statistic:                     157.6\n",
       "Date:                Thu, 01 May 2025   Prob (F-statistic):          5.72e-269\n",
       "Time:                        08:39:33   Log-Likelihood:                -5880.5\n",
       "No. Observations:                 731   AIC:                         1.181e+04\n",
       "Df Residuals:                     705   BIC:                         1.193e+04\n",
       "Df Model:                          25                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "================================================================================\n",
       "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------\n",
       "const         1467.0996    237.017      6.190      0.000    1001.755    1932.444\n",
       "yr            2017.5574     57.805     34.903      0.000    1904.067    2131.047\n",
       "holiday       -293.6274    206.548     -1.422      0.156    -699.150     111.895\n",
       "workingday     307.8027    106.723      2.884      0.004      98.270     517.336\n",
       "temp          2994.1135   1349.265      2.219      0.027     345.055    5643.172\n",
       "atemp         1716.3066   1454.226      1.180      0.238   -1138.825    4571.439\n",
       "hum          -1560.0279    287.087     -5.434      0.000   -2123.676    -996.380\n",
       "windspeed    -2846.9942    411.603     -6.917      0.000   -3655.108   -2038.880\n",
       "season_2       887.3336    164.562      5.392      0.000     564.243    1210.424\n",
       "season_3       832.0276    169.183      4.918      0.000     499.864    1164.191\n",
       "season_4      1495.1711    114.763     13.028      0.000    1269.853    1720.489\n",
       "mnth_2         150.2921    130.537      1.151      0.250    -105.995     406.580\n",
       "mnth_3         554.3180    133.061      4.166      0.000     293.075     815.561\n",
       "mnth_4         455.8258    189.424      2.406      0.016      83.924     827.728\n",
       "mnth_5         731.4736    192.963      3.791      0.000     352.622    1110.325\n",
       "mnth_6         502.7560    164.346      3.059      0.002     180.089     825.423\n",
       "mnth_8         432.9388    132.744      3.261      0.001     172.319     693.559\n",
       "mnth_9        1022.8398    122.773      8.331      0.000     781.795    1263.884\n",
       "mnth_10        606.9923    126.412      4.802      0.000     358.803     855.182\n",
       "weekday_1      -95.6125    108.992     -0.877      0.381    -309.601     118.376\n",
       "weekday_3       70.7323    106.736      0.663      0.508    -138.826     280.290\n",
       "weekday_4       74.9387    106.757      0.702      0.483    -134.662     284.539\n",
       "weekday_5      126.0524    107.145      1.176      0.240     -84.310     336.415\n",
       "weekday_6      438.3308    106.312      4.123      0.000     229.605     647.057\n",
       "weathersit_2  -458.7666     76.567     -5.992      0.000    -609.093    -308.440\n",
       "weathersit_3 -1964.3498    196.429    -10.000      0.000   -2350.005   -1578.695\n",
       "==============================================================================\n",
       "Omnibus:                      125.403   Durbin-Watson:                   1.194\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              345.658\n",
       "Skew:                          -0.862   Prob(JB):                     8.74e-76\n",
       "Kurtosis:                       5.894   Cond. No.                         124.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Put your code here\n",
    "df = pd.read_csv(\"day.csv\").drop(columns=[\"dteday\", \"instant\", \"casual\", \"registered\"])\n",
    "df[['season', 'weathersit', 'mnth', 'weekday']] = df[['season', 'weathersit', 'mnth', 'weekday']].astype('category')\n",
    "df = pd.get_dummies(df, drop_first=True).replace({False: 0, True: 1})\n",
    "y = df['cnt']\n",
    "X = df.drop(columns=['cnt', 'mnth_7', 'mnth_11', 'mnth_12', 'weekday_2'])\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(y, X).fit()\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 3.7 (3 points)**: Discuss the difference in fit quality between the two fits. Did the second fit (with \"least important\" feature removed) outperform or underperform compared to the other? Describe how you evaluated the quality based on the fit statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> The second model performed slightly better than the first one. It had a higher adjusted R^2 (0.843 vs 0.842) and a stronger F stat (157.6 vs 140.3), showing it was more efficient and still highly significant. It also had lower AIC and BIC values, meaning it used fewer features without losing accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 3.8 (5 points)**: What does this all mean? What is your best R-squared value or adjusted R-squared value? What can we say about the data and how it might be related? What does it mean when people say that correlation is not causation? Please try to answer these questions the best that you can in a paragraph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> The best adjusted R^2 val was 0.843, which means about 84% of the variation in bike rentals can be explained by the features in our model. This implejis theres a strong relationship between things like temperature, weather, and season, and how many people rent bikes. Howvwere, bc two things are related doesnt mean one causes the other, andis what people mean when they say correlation isnt causation. For example, more people might rent bikes when its warm out, but warm weather doesnt directly cause rentals (it just happens at the same time as other things, like people being off school or work). Our model fits well, but we cant say for sure what actually causes people to rent more bikes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository!**\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository using the commit message \"**Committing Part 3**\", and push the changes to GitHub.\n",
    "\n",
    "\n",
    "If committing and/or pushing isn't working for you, write down the complete commands in this cell that would have committed your changes (with the commit message) and pushed them to GitHub.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Part 4: Support vector machine (SVM) classification (34 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the exam, we will use a support vector machine (SVM) classifier to identify emails as spam or not based on various email features. We will be using the UC Irvine Machine Learning Repository Spambase Dataset. More info about this dataset can be found here https://archive.ics.uci.edu/dataset/94/spambase. \n",
    "\n",
    "To get started, download the `spambasedata.csv` file from the link below (or D2L) and place it in the same directory as your notebook. \n",
    "\n",
    "`https://raw.githubusercontent.com/gambre11/CMSE202/refs/heads/main/spambasedata.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 4.1 (2 points)**: Read in the `spambase.csv` dataset into a `Pandas` `DataFrame` and display the first few rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put your code for Question 4.1 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 4.2 (2 points)**: The following cell contains a list with the names of every column in this dataset. Add these column names to your data that you have read in, and display your data again. Double check and make sure you are not losing any data when doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\", \n",
    "                      \"word_freq_our\", \"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\", \n",
    "                      \"word_freq_order\", \"word_freq_mail\", \"word_freq_receive\", \"word_freq_will\", \n",
    "                      \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\", \"word_freq_free\", \n",
    "                      \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\", \n",
    "                      \"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\", \"word_freq_hp\", \n",
    "                      \"word_freq_hpl\", \"word_freq_george\", \"word_freq_650\", \"word_freq_lab\", \"word_freq_labs\", \n",
    "                      \"word_freq_telnet\", \"word_freq_857\", \"word_freq_data\", \"word_freq_415\", \"word_freq_85\", \n",
    "                      \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\", \"word_freq_direct\",\n",
    "                      \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\", \"word_freq_re\", \n",
    "                      \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\", \"char_freq_;\", \"char_freq_(\", \n",
    "                      \"char_freq_[\", \"char_freq_!\", \"char_freq_$\", \"char_freq_hash\", \"capital_run_length_average\", \n",
    "                      \"capital_run_length_longest\", \"capital_run_length_total\", \"spam\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 4.3 (2 points)**: How many rows of data are there? How many emails are marked as spam and what is this percentage compared to the entire dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put your work and answer to Question 4.3 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 4.4 (2 points)**: Our goal is to classify the `spam` of the email given the other 57 features. Create a variable with all of the columns of the `DataFrame` except for `spam`,  and another variable with just the `spam` column of the `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put your code for Question 4.4 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is properly loaded into Python, we need to perform a **train-test-split** so that we can build our SVM classifier and test it.\n",
    "\n",
    "&#9989; **Question 4.5 (4 points)**: Use the `train_test_split()` method from `sklearn.model_selection` like we did in class. Use a `train_size` of `0.78` and `random_state` of `1616`. You should now have training features, testing features, training labels, and testing labels. Finally, **print the shape of your training features, training labels, testing features, and testing labels** to verify that your train-test-split did what it was supposed to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put your code for Question 4.5 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 4.6 (6 points)**: Fit an SVM classifier (using the `sklearn` `SVC` class) to the dataset. Use a `linear` kernel and set the hyper-parameter to be `C=0.001.` Then **fit the SVM using your training set** and use the resulting SVM to **predict the labels for the testing set** so you get predicted labels for the testing set. Finally, **print the fit statistics** using the `confusion_matrix()` and `classification_report()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put your code for Question 4.6 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 4.7 (2 points)**: Create two more SVM classifiers and test them by repeating your work from Question 4.6, but this time set the hyper-parameters to be `C=0.01.` and `C=0.1.` Again, **print the fit statistics** using the `confusion_matrix()` and `classification_report()` methods. Note that this might take 30 seconds to a minute to run. If it takes longer than 2 minutes, flag an instructor for help. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Put your code for Question 4.7 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 4.8 (6 points)**: Interpret the outputs of your classification reports and the confusion matrices. In particular: \n",
    "\n",
    "* From your classification report, talk about what the **precision**, **recall**, and **f1-score** mean in this context. Do not just give definitions.\n",
    "\n",
    "* From your confusion matrix, what do each of the four numbers represent in this context. Again do not give a broad definition.\n",
    "\n",
    "* How does your accuracy change as C changes? Why does this happen? What is the parameter C and what does increasing or decreasing its value represent when classifying data using SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> **Do This - Erase the contents of this cell an put your answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "&#9989; **Question 4.9 (1 point)**: Suppose we wanted to try fitting a Support Vector Classifier for multiple choices of the kernel function and multiple choices for the values of the hyperparameter(s) (instead of just using a `linear` kernel with one value of `C`). We could write code with nested for loops to repeat the procedure with every combination of kernel function and hyperparameter value(s) we wanted to try. Name a method built into sklearn that will do this automatically. (We used this on an in-class assignment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> **Do This - Erase the contents of this cell an put your answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "&#9989; **Question 4.10 (1 point)**: When filtering out spam emails, we want to minimize filtering out emails that may be important. What could we do to prioritize this? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> **Do This - Erase the contents of this cell an put your answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 4.11 (3 points)**: Both of the images below show the same two-dimensional dataset with two classes (solid blue squares and unfilled red circles) along with the decision boundary of a linear classifier. One classifier was generated via the Perceptron Learning Algorithm, and the other used a Support Vector Classifier. Which one is which? **Justify your answer!**\n",
    "\n",
    "Classifier A          | | Classifier B\n",
    ":-------------------------:|:---:|:-------------------------:\n",
    "![](https://i.ibb.co/R2BBsDC/Datapoints1-A.png)  | |  ![](https://i.ibb.co/mb9vcq4/Datapoints1-B.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> **Do This - Erase the contents of this cell an put your answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 4.12 (3 points)**: Both of the images below show the same two-dimensional dataset with two classes (solid blue squares and unfilled red circles) along with the decision boundary of a linear Support Vector Classifier. One used the hyperparameter `C = 0.1`, and the other used the hyperparameter `C = 1000`. Which one is which? **Justify your answer!**\n",
    "\n",
    "Classifier X          | | Classifier Y\n",
    ":-------------------------:|:---:|:-------------------------:\n",
    "![](https://i.ibb.co/7pPCRwh/Datapoints2-A.png)  | |  ![](https://i.ibb.co/LSMBXzd/Datapoints2-B.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### &#128721; STOP\n",
    "**Pause to commit your changes to your Git repository!**\n",
    "\n",
    "Take a moment to save your notebook, commit the changes to your Git repository using the commit message \"**Committing Part 4**\", and push the changes to GitHub.\n",
    "\n",
    "\n",
    "If committing and/or pushing isn't working for you, write down the complete commands in this cell that would have committed your changes (with the commit message) and pushed them to GitHub.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Part 5: Principal Component Analysis (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989; **Question 5 (5 points)**: What was the point of doing Principal Component Analysis on our face data from our day 23 ICA? What does Principal Component Analysis help us do? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=+3>&#9998;</font> **Do This - Erase the contents of this cell an put your answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You're done! Congrats on finishing your CMSE 202 Final!\n",
    "\n",
    "Make sure all of your changes to your repository are committed and pushed to GitHub (or that you wrote down the commands that would have done that after each part). Also upload a copy of this notebook to the dropbox on D2L in case something went wrong with your repository or if you couldn't get the repository to work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
